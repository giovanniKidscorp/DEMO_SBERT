{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f04254b",
   "metadata": {},
   "source": [
    "## Uso YT dlp para la demo, scrapping basico de metadatos. Quiza tengo que pasar a una api oficial para que sea mas estable o tener acceso a mas datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0157b095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OK: BabyBus - Canciones Infantiles & Videos para Ni√±os (3 videos)\n",
      "    OK: Smile and Learn - Espa√±ol (3 videos)\n",
      "    OK: Creativos Kids (2 videos)\n",
      "    OK: Juguetes y Colores (10 videos)\n",
      "    OK: Genevieve's Playhouse - Learning Videos for Kids (2 videos)\n",
      "    OK: Gaming With GTA  (10 videos)\n",
      "    OK: Videos de juguetes Paw Patrol en espa√±ol (10 videos)\n",
      "\n",
      "‚ú® Proceso terminado en 4.43 segundos.\n",
      "üìÇ Datos guardados en 'dataset_canales.json'.\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "import json\n",
    "import time\n",
    "\n",
    "lista_canales = [\n",
    "    \"https://www.youtube.com/@BabyBusES\",\n",
    "    \"https://www.youtube.com/@SmileandLearnEspa%C3%B1ol\",\n",
    "    \"https://www.youtube.com/@creativoskids\",\n",
    "    \"https://www.youtube.com/@JuguetesyColores\",\n",
    "    \"https://www.youtube.com/@GenevievesPlayhouse\",\n",
    "    \"https://www.youtube.com/@GamingWithGTA00\",\n",
    "    \"https://www.youtube.com/@videosdejuguetespawpatrole5118\",\n",
    "]\n",
    "\n",
    "def scrapear_lista_canales(urls):\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'extract_flat': True,  # (solo metadatos)\n",
    "        'playlistend': 10,     # L√≠mite de videos\n",
    "        'ignoreerrors': True,  \n",
    "        'no_warnings': True,\n",
    "        # 'sleep_interval': 1,\n",
    "    }\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        \n",
    "        for index, url in enumerate(urls):\n",
    "            \n",
    "            try:\n",
    "                # Extraemos la info\n",
    "                info = ydl.extract_info(url, download=False)\n",
    "                \n",
    "                # A veces yt-dlp devuelve None si la url est√° mal\n",
    "                if not info:\n",
    "                    print(f\" No se pudo obtener info de {url}\")\n",
    "                    continue\n",
    "\n",
    "                # Estructuramos los datos\n",
    "                datos = {\n",
    "                    \"id_canal\": info.get('id'),\n",
    "                    \"nombre\": info.get('uploader') or info.get('title'),\n",
    "                    \"descripcion\": info.get('description', ''),\n",
    "                    \"suscriptores\": info.get('channel_follower_count'),\n",
    "                    \"url_origen\": url,\n",
    "                    # Juntamos los t√≠tulos de los videos en una lista simple\n",
    "                    \"videos_recientes\": []\n",
    "                }\n",
    "\n",
    "                # Extraer videos (validando que existan entradas)\n",
    "                if 'entries' in info:\n",
    "                    for video in info['entries']:\n",
    "                        if video and video.get('title'):\n",
    "                            datos[\"videos_recientes\"].append(video.get('title'))\n",
    "                \n",
    "                resultados.append(datos)\n",
    "                print(f\"    OK: {datos['nombre']} ({len(datos['videos_recientes'])} videos)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Error en este canal: {str(e)}\")\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# --- EJECUCI√ìN ---\n",
    "\n",
    "start = time.time()\n",
    "data_scraped = scrapear_lista_canales(lista_canales)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\n‚ú® Proceso terminado en {end - start:.2f} segundos.\")\n",
    "\n",
    "# 2. Guardar en un archivo JSON (Esto es lo que usar√° tu IA despu√©s)\n",
    "nombre_archivo = \"dataset_canales.json\"\n",
    "with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_scraped, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"üìÇ Datos guardados en '{nombre_archivo}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab89ec",
   "metadata": {},
   "source": [
    "## Recordar, meter los embeddings en una base de datos vectorial!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2525c1fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PreTrainedModel' from 'transformers' (c:\\Users\\Giovanni Pitta\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mdataset_canales.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Giovanni Pitta\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\__init__.py:15\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Giovanni Pitta\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Giovanni Pitta\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     AutoConfig,\n\u001b[32m     23\u001b[39m     AutoModelForSequenceClassification,\n\u001b[32m     24\u001b[39m     AutoTokenizer,\n\u001b[32m     25\u001b[39m     PretrainedConfig,\n\u001b[32m     26\u001b[39m     PreTrainedModel,\n\u001b[32m     27\u001b[39m     PreTrainedTokenizer,\n\u001b[32m     28\u001b[39m     is_torch_npu_available,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PreTrainedModel' from 'transformers' (c:\\Users\\Giovanni Pitta\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "try:\n",
    "    with open('dataset_canales.json', 'r', encoding='utf-8') as f:\n",
    "        datos_canales = json.load(f)\n",
    "    print(f\"   --> {len(datos_canales)} canales cargados.\")\n",
    "except FileNotFoundError:\n",
    "    print(\" Error: No encontr√© el archivo 'dataset_canales.json'. Ejecuta el scraper primero.\")\n",
    "    exit()\n",
    "\n",
    "# El modelo necesita un solo texto por canal para entender de qu√© trata.\n",
    "corpus_textos = []\n",
    "\n",
    "for canal in datos_canales:\n",
    "    # Unimos los titulos de videos con comas\n",
    "    txt_videos = \", \".join(canal.get('videos_recientes', []))\n",
    "    \n",
    "    # Creamos el texto maestro para la IA\n",
    "    # \"Nombre: [X]. Descripci√≥n: [Y]. Contenido reciente: [Z]\"\n",
    "    texto_full = f\"Canal: {canal['nombre']}. Descripci√≥n: {canal['descripcion']}. Temas de videos: {txt_videos}\"\n",
    "    \n",
    "    corpus_textos.append(texto_full)\n",
    "\n",
    "# --- INDEXACI√ìN  ---\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "embeddings_db = model.encode(corpus_textos, convert_to_tensor=True)\n",
    "\n",
    "def buscar(query_usuario, top_k=3):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Vectorizamos lo que escribi√≥ el usuario\n",
    "    query_embedding = model.encode(query_usuario, convert_to_tensor=True)\n",
    "    \n",
    "    # Buscamos similitud\n",
    "    hits = util.semantic_search(query_embedding, embeddings_db, top_k=top_k)\n",
    "    \n",
    "    print(f\" B√∫squeda: '{query_usuario}' ({time.time() - start_time:.4f} seg)\")\n",
    "    \n",
    "    for hit in hits[0]:\n",
    "        score = hit['score']\n",
    "        id_canal = hit['corpus_id'] # El √≠ndice en la lista original\n",
    "        canal_original = datos_canales[id_canal] # Recuperamos el objeto JSON original\n",
    "        \n",
    "        # Filtro de calidad\n",
    "        if score < 0.25: \n",
    "            continue\n",
    "            \n",
    "        print(f\" Match: {score:.2f} | {canal_original['nombre']}\")\n",
    "        # por que hizo match?\n",
    "        print(f\"      (Contexto: {canal_original['videos_recientes'][:2]}...)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- PRUEBAS ---\n",
    "# Ahora simulamos las queries libres de tus clientes\n",
    "buscar(\"videos\")\n",
    "buscar(\"aprender\")\n",
    "buscar(\"juegos\")\n",
    "buscar(\"tutoriales\")\n",
    "buscar(\"dibujos animados para bebes\")\n",
    "buscar(\"ni√±os creativos\")\n",
    "buscar(\"diversion\")\n",
    "buscar(\"jugar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22227040",
   "metadata": {},
   "source": [
    "\n",
    "#### *VOLVAMOS A INTENTAR, USANDO LA BASE DE DATOS QUE YA TENEMOS EN KIDSCORP YT*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "474935c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle  # <--- Librer√≠a para guardar archivos\n",
    "# ... tus otros imports ...\n",
    "\n",
    "# Nombre del archivo donde guardaremos los \"cerebros\"\n",
    "ARCHIVO_EMBEDDINGS = \"embeddings_cache.pkl\"\n",
    "\n",
    "# ... (Tu funci√≥n obtener_datos sigue igual) ...\n",
    "\n",
    "# --- L√ìGICA DE CARGA INTELIGENTE ---\n",
    "def cargar_o_generar_embeddings(df):\n",
    "    # 1. ¬øYa existe el archivo guardado?\n",
    "    if os.path.exists(ARCHIVO_EMBEDDINGS):\n",
    "        print(f\"üíæ Cargando embeddings desde '{ARCHIVO_EMBEDDINGS}'...\")\n",
    "        with open(ARCHIVO_EMBEDDINGS, 'rb') as f:\n",
    "            datos_guardados = pickle.load(f)\n",
    "            # Verificamos que coincidan con los datos actuales (opcional pero recomendado)\n",
    "            if len(datos_guardados) == len(df):\n",
    "                print(\"‚úÖ Embeddings cargados exitosamente.\")\n",
    "                return datos_guardados\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Los datos cambiaron. Recalculando embeddings...\")\n",
    "\n",
    "    # 2. Si no existe o cambiaron los datos, generamos de cero\n",
    "    print(\"‚ö° Generando nuevos Embeddings (esto tardar√°)...\")\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    \n",
    "    # Aseg√∫rate de haber creado la columna 'texto_para_ia' antes\n",
    "    embeddings = model.encode(df['texto_para_ia'].tolist(), convert_to_tensor=True)\n",
    "    \n",
    "    # 3. Guardamos para la pr√≥xima\n",
    "    print(f\"üíæ Guardando en '{ARCHIVO_EMBEDDINGS}' para el futuro...\")\n",
    "    with open(ARCHIVO_EMBEDDINGS, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c9b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Descargando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giovanni Pitta\\AppData\\Local\\Temp\\ipykernel_17016\\3801974127.py:30: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 487.30it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Generando nuevos Embeddings (esto tardar√°)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 664.41it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Guardando en 'embeddings_cache.pkl' para el futuro...\n",
      " Proceso terminado en 275.63 segundos.\n",
      " Canales procesados: 10000\n",
      "\n",
      "üîé Buscando: 'videos de manualidades para ni√±os'\n",
      "   ‚òÖ 0.79 | KiKi-RiKi Videos Infantiles para ni√±os\n",
      "   ‚òÖ 0.74 | Juntines Planes\n",
      "   ‚òÖ 0.73 | Marie Leiner\n",
      "\n",
      "üîé Buscando: 'gameplay de minecraft'\n",
      "   ‚òÖ 0.85 | Pober\n",
      "   ‚òÖ 0.80 | Omarcito\n",
      "   ‚òÖ 0.80 | El ba√∫l de Jaime\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carga las variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def obtener_datos():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=os.getenv(\"DB_HOST\"),\n",
    "            database=os.getenv(\"DB_NAME\"),\n",
    "            user=os.getenv(\"DB_USER\"),\n",
    "            password=os.getenv(\"DB_PASS\"),\n",
    "            port=\"5432\",\n",
    "            sslmode=\"require\"\n",
    "        )\n",
    "        \n",
    "        query = \"\"\"\n",
    "            SELECT channel_title, channel_description, channel_bs_ch_keywords \n",
    "            FROM ods.tbl_canales \n",
    "            LIMIT 10000;\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al conectar: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        if conn: conn.close()\n",
    "\n",
    "# --- PROCESO PRINCIPAL ---\n",
    "start = time.time()\n",
    "\n",
    "# 1. Obtener Datos\n",
    "df_canales = obtener_datos()\n",
    "\n",
    "if df_canales.empty:\n",
    "    print(\" No se descargaron datos.\")\n",
    "    exit()\n",
    "\n",
    "# Rellenamos los NULLs con texto vac√≠o para que no explote la IA\n",
    "df_canales = df_canales.fillna('')\n",
    "\n",
    "# 3. CREAR TEXTO ENRIQUECIDO\n",
    "# Juntamos T√≠tulo + Descripci√≥n + Keywords en una sola columna para la IA\n",
    "df_canales['texto_para_ia'] = (\n",
    "    \"Canal: \" + df_canales['channel_title'] + \". \" +\n",
    "    \"Descripci√≥n: \" + df_canales['channel_description'] + \". \" +\n",
    "    \"Keywords: \" + df_canales['channel_bs_ch_keywords']\n",
    ")\n",
    "\n",
    "# 4. VECTORIZAR\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# 2. En lugar de llamar a model.encode directo, llamas a la funci√≥n inteligente\n",
    "embeddings_sb = cargar_o_generar_embeddings(df_canales)\n",
    "\n",
    "print(f\" Proceso terminado en {time.time() - start:.2f} segundos.\")\n",
    "print(f\" Canales procesados: {len(df_canales)}\")\n",
    "# --- TEST DE B√öSQUEDA ---\n",
    "def buscar(query_usuario):\n",
    "    query_vec = model.encode(query_usuario, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_vec, embeddings_sb, top_k=100)\n",
    "    \n",
    "    for hit in hits[0]:\n",
    "        idx = hit['corpus_id']\n",
    "        info_canal = df_canales.iloc[idx]\n",
    "        print(f\"   ‚òÖ {hit['score']:.2f} | {info_canal['channel_title']}\")\n",
    "\n",
    "# Pru√©balo aqu√≠ mismo\n",
    "buscar(\"videos de manualidades para ni√±os\")\n",
    "buscar(\"gameplay de minecraft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b090169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Buscando: 'canciones'\n",
      "   ‚òÖ 0.65 | Canciones De Amor\n",
      "   ‚òÖ 0.64 | Sarah Cabello\n",
      "   ‚òÖ 0.63 | Lyrics Music\n",
      "\n",
      "üîé Buscando: 'aprender idiomas'\n",
      "   ‚òÖ 0.62 | Academia do Ingl√™s\n",
      "   ‚òÖ 0.59 | Ingl√™s F√°cil com Professor Marcondes\n",
      "   ‚òÖ 0.58 | Listen & Learn English\n",
      "\n",
      "üîé Buscando: 'poopie'\n",
      "   ‚òÖ 0.53 | Pocoyo üáßüá∑ Portugu√™s Brasil - Canal Oficial\n",
      "   ‚òÖ 0.52 | OPPA K√äKI\n",
      "   ‚òÖ 0.51 | Morphle o Meu Pet Magico\n",
      "\n",
      "üîé Buscando: 'mrbeast'\n",
      "   ‚òÖ 0.58 | MrAerotech\n",
      "   ‚òÖ 0.51 | MrBeast Gaming en Espa√±ol\n",
      "   ‚òÖ 0.46 | Mantovani\n"
     ]
    }
   ],
   "source": [
    "buscar(\"canciones\")\n",
    "buscar(\"aprender idiomas\")\n",
    "buscar(\"poopie\")\n",
    "buscar(\"mrbeast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35e759",
   "metadata": {},
   "source": [
    "### Proximos pasos :\n",
    "#### Mejorar la recopilacion de data. Usar la base de datos que tenemos, y descargar descripciones ultimos 10 videos y si podemos integrar el trafico de cada canal al sistema. Osea, cuantos seguidores tiene (mas asincronico) y cuanto trafico relativo tuvo en el ultimo tiempo asi nos aseguramos de que tengan un buen potencial de generacion de impresiones.\n",
    "#### Convertir en clases y modularizar.\n",
    "#### Pensar algun metodo de benchmark.\n",
    "#### Investigar que hace la funcion semantic search (seguro es cosine similarity pero quiza hay una variante mas util para este problema).\n",
    "#### Determinar modos de busqueda, osea si van a ser una palabra solo, varias palabras, ambas. Ver lo de la negativa (como hago para notBuscar, funcion aparte o puedo hacerlo todo junto?)(no quiero videos de futbol, por ejemplo)\n",
    "\n",
    "\n",
    "<span style=\"color:green\">Notas anteriores:</span>\n",
    "\n",
    "<span style=\"color:white\">Podria hacer un funnel con embeddings - ReRankers, pero creo que para este caso no es muy necesario (se usa para evitar fallas en entender el contexto, el reranker \"lee la frase completa\")\n",
    "\n",
    "Quiza pifia con nombre inventados, por ejemplo si alguien le pinta buscar mrbeast puede no salir (solucion hibrida con un tf-idf, para resultados mas exactos)</span>\n",
    "#### Definir estructura de datos\n",
    "#### Integrar el grafo que tenemos (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bea3f5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
